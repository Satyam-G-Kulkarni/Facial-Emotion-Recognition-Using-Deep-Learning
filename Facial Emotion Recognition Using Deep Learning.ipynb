{"cells":[{"cell_type":"markdown","metadata":{"id":"vncDsAP0Gaoa"},"source":["# **Project Name**    - Facial Emotion Recognition Using Deep Learning\n","\n"]},{"cell_type":"markdown","metadata":{"id":"beRrZCGUAJYm"},"source":["##### **Project Type**    - Classification\n","##### **Contribution**    - Individual"]},{"cell_type":"markdown","metadata":{"id":"FJNUwmbgGyua"},"source":["# **Project Summary -**"]},{"cell_type":"markdown","metadata":{"id":"F6v_1wHtG2nS"},"source":["**Overview**\n","\n","The project aims to develop a web application for real-time emotion recognition using a pre-trained deep learning model. The application consists of a Flask server running in a Colab environment, which receives images from a JavaScript frontend, processes them using OpenCV and TensorFlow/Keras, and predicts the predominant emotion displayed in the images.\n","\n","Components and Functionality\n","Flask Server\n","\n","Purpose: Acts as the backend server to handle image uploads, process them for emotion recognition, and serve predictions.\n","Framework: Utilizes Flask, a lightweight Python web framework, for handling HTTP requests and responses.\n","\n","**Endpoints:**\n","\n","/predict_emotion: Receives POST requests containing images, preprocesses them, predicts emotions using a pre-trained TensorFlow/Keras model, and returns the predicted emotion along with confidence scores as JSON responses.\n","/: Serves a basic HTML page to display the real-time video feed and interact with the prediction functionality.\n","Emotion Recognition Model\n","\n","Type: Utilizes a pre-trained deep learning model implemented using TensorFlow/Keras.\n","Functionality: Predicts emotions from facial expressions captured in images.\n","Preprocessing: Converts images to grayscale, resizes them to match the model input size, normalizes pixel values, and expands dimensions for model compatibility.\n","Output: Predicts one of seven emotions (angry, disgust, fear, happy, neutral, sad, surprise) along with confidence levels.\n","JavaScript Frontend\n","\n","Purpose: Provides a user interface for uploading images and displaying real-time video feed from the webcam.\n","Components: Utilizes HTML, CSS for styling, and JavaScript (jQuery) for frontend interactions.\n","Functionality:\n","Allows users to select an image file from their device.\n","Sends the selected image to the Flask server for emotion prediction via AJAX POST requests.\n","Displays the predicted emotion and confidence level returned by the server on the webpage.\n","\n","**Integration and Deployment**\n","\n","Environment: Developed and tested in a Google Colab environment for seamless integration of Python-based backend (Flask) and deep learning libraries (TensorFlow/Keras).\n","\n","Deployment: Utilizes Ngrok for creating a secure tunnel to expose the Flask server running on localhost to the internet, enabling access from external devices and testing across different platforms.\n","Benefits and Use Cases\n","\n","Real-Time Feedback: Provides immediate feedback on detected emotions, suitable for interactive applications requiring emotion recognition, such as virtual assistants, customer sentiment analysis, and educational tools.\n","Flexible Deployment: Can be deployed on various platforms, including local servers, cloud environments, or integrated into existing web applications.\n","\n","Educational Purpose: Offers a practical example of integrating computer vision and deep learning techniques into web development, enhancing learning and experimentation with modern AI technologies.\n","\n","Future Enhancements\n","\n","Performance Optimization: Explore methods to improve real-time processing speed and efficiency, potentially leveraging hardware acceleration (GPU).\n","User Interface Improvements: Enhance frontend design and user experience with additional features like real-time video streaming and multi-face emotion detection.\n","\n","Model Fine-Tuning: Continuously refine the emotion recognition model to improve accuracy across different demographics and facial expressions.\n","\n","**Conclusion**\n","\n","The real-time emotion recognition web application demonstrates the integration of Flask, TensorFlow/Keras, and JavaScript to create a responsive and interactive user experience. It serves as a foundational project for exploring AI-driven applications in web development, focusing on emotion analysis through facial expressions.\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"w6K7xa23Elo4"},"source":["# **GitHub Link -**"]},{"cell_type":"markdown","metadata":{"id":"h1o69JH3Eqqn"},"source":["https://github.com/Satyam-G-Kulkarni/Facial-Emotion-Recognition-Using-Deep-Learning"]},{"cell_type":"markdown","metadata":{"id":"yQaldy8SH6Dl"},"source":["# **Problem Statement**\n"]},{"cell_type":"markdown","metadata":{"id":"DpeJGUA3kjGy"},"source":["Accurately recognizing human emotions through facial expressions is essential for enhancing human-computer interaction, mental health monitoring, customer service, and other empathetic machine responses. Despite advancements in AI, creating a robust and reliable real-time facial emotion recognition system remains challenging due to the variability in expressions and diverse image conditions.\n","\n","DeepFER aims to develop an advanced system that accurately identifies and classifies seven distinct emotions—angry, sad, happy, fear, neutral, disgust, and surprise—using Convolutional Neural Networks (CNNs) and Transfer Learning. The project seeks to bridge the gap between AI research and practical applications, enhancing machine interactions with humans.\n","\n","Key objectives include:\n","\n","Data Collection and Preprocessing: Assemble a diverse dataset and apply data augmentation techniques.\n","\n","Model Development: Design a CNN architecture and fine-tune pre-trained models.\n","\n","Training and Evaluation: Train the model and evaluate its performance.\n","\n","Real-Time Processing: Develop algorithms for real-time emotion recognition.\n","\n","Application Development: Create a user-friendly application for various domains.\n","\n","Performance Optimization: Enhance model efficiency and speed.\n","\n","Documentation and Reporting: Document the development process and findings.\n","\n","Deployment and Testing: Test the system in real-world scenarios and gather feedback."]},{"cell_type":"markdown","metadata":{"id":"mDgbUHAGgjLW"},"source":["# **General Guidelines** : -  "]},{"cell_type":"markdown","metadata":{"id":"ZrxVaUj-hHfC"},"source":["1.   Well-structured, formatted, and commented code is required.\n","2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n","     \n","     The additional credits will have advantages over other students during Star Student selection.\n","       \n","             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n","                       without a single error logged. ]\n","\n","3.   Each and every logic should have proper comments.\n","4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n","        \n","\n","```\n","# Chart visualization code\n","```\n","            \n","\n","*   Why did you pick the specific chart?\n","*   What is/are the insight(s) found from the chart?\n","* Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","5. You have to create at least 15 logical & meaningful charts having important insights.\n","\n","\n","[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n","\n","U - Univariate Analysis,\n","\n","B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n","\n","M - Multivariate Analysis\n"," ]\n","\n","\n","\n","\n","\n","6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n","\n","\n","*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n","\n","\n","*   Cross- Validation & Hyperparameter Tuning\n","\n","*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n","\n","*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"O_i_v8NEhb9l"},"source":["# ***Let's Begin !***"]},{"cell_type":"markdown","source":["## **Data Loading and Preprocessing**"],"metadata":{"id":"TnqF7WmjOlGG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"AkrRSSSdmZK2"},"outputs":[],"source":["import tensorflow as tf\n","import os\n","import pandas as pd\n","from tensorflow.keras import layers\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.model_selection import train_test_split\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F08bsG9ijGD6"},"outputs":[],"source":["# Data directories and parameters\n","train_dir = '/content/drive/MyDrive/Projects/Deep Learning for Computer Vision/Data/images/train'\n","validation_dir = '/content/drive/MyDrive/Projects/Deep Learning for Computer Vision/Data/images/validation'\n","Classes = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n","img_size = 128\n","batch_size = 32"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FQEfs_d6jWhR"},"outputs":[],"source":["# Get the list of all image files in the training data directory\n","all_image_files = []\n","all_labels = []\n","\n","for class_label in Classes:\n","    class_path = os.path.join(train_dir, class_label)\n","    if os.path.exists(class_path):\n","        image_files = [os.path.join(class_path, img) for img in os.listdir(class_path) if img.endswith(\".jpg\")]\n","        all_image_files.extend(image_files)\n","        all_labels.extend([class_label] * len(image_files))\n","\n","# Create a DataFrame with file paths and labels\n","df = pd.DataFrame({'filepath': all_image_files, 'label': all_labels})\n","\n","# Split the data into training and validation sets\n","train_df, validation_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9431,"status":"ok","timestamp":1719463707153,"user":{"displayName":"Satyam Kulkarni","userId":"17502473525507827216"},"user_tz":-330},"id":"vUPBXwQ1r4NJ","outputId":"5291f924-1997-4fcd-bdab-cfa2ed532b6b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 23056 validated image filenames belonging to 7 classes.\n","Found 5765 validated image filenames belonging to 7 classes.\n"]}],"source":["# Data generators for training and validation with reduced augmentation\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=10,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    shear_range=0.1,\n","    zoom_range=0.1,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","validation_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# Data generators\n","train_generator = train_datagen.flow_from_dataframe(\n","    train_df,\n","    x_col='filepath',\n","    y_col='label',\n","    target_size=(img_size, img_size),\n","    batch_size=batch_size,\n","    class_mode='sparse',\n","    classes=Classes\n",")\n","\n","validation_generator = validation_datagen.flow_from_dataframe(\n","    validation_df,\n","    x_col='filepath',\n","    y_col='label',\n","    target_size=(img_size, img_size),\n","    batch_size=batch_size,\n","    class_mode='sparse',\n","    classes=Classes\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"VfCC591jGiD4"},"source":["## ***7. ML Model Implementation***"]},{"cell_type":"markdown","metadata":{"id":"OB4l2ZhMeS1U"},"source":["### ML Model - Deep Learning CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":217215,"status":"ok","timestamp":1719482941243,"user":{"displayName":"Satyam Kulkarni","userId":"17502473525507827216"},"user_tz":-330},"id":"7aq2bJNqbUmf","outputId":"f36a90a8-8ff5-442c-dbf8-a329df51ff19"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n","9406464/9406464 [==============================] - 0s 0us/step\n","Epoch 1/20\n","721/721 [==============================] - 5759s 8s/step - loss: 1.5162 - accuracy: 0.4559 - val_loss: 1.5711 - val_accuracy: 0.4539\n","Epoch 2/20\n","721/721 [==============================] - 1469s 2s/step - loss: 1.1904 - accuracy: 0.5576 - val_loss: 1.8743 - val_accuracy: 0.4805\n","Epoch 3/20\n","721/721 [==============================] - 1481s 2s/step - loss: 1.1305 - accuracy: 0.5807 - val_loss: 1.8567 - val_accuracy: 0.4810\n","Epoch 4/20\n","721/721 [==============================] - 1498s 2s/step - loss: 1.0998 - accuracy: 0.5890 - val_loss: 2.0996 - val_accuracy: 0.4533\n","Epoch 5/20\n","721/721 [==============================] - 1480s 2s/step - loss: 1.0792 - accuracy: 0.5994 - val_loss: 1.3258 - val_accuracy: 0.5547\n","Epoch 6/20\n","721/721 [==============================] - 1452s 2s/step - loss: 1.0536 - accuracy: 0.6076 - val_loss: 2.1275 - val_accuracy: 0.4644\n","Epoch 7/20\n","721/721 [==============================] - 1454s 2s/step - loss: 1.0390 - accuracy: 0.6147 - val_loss: 1.3596 - val_accuracy: 0.5341\n","Epoch 8/20\n","721/721 [==============================] - 1443s 2s/step - loss: 1.0219 - accuracy: 0.6206 - val_loss: 2.1329 - val_accuracy: 0.4869\n","Epoch 9/20\n","721/721 [==============================] - 1492s 2s/step - loss: 1.0036 - accuracy: 0.6294 - val_loss: 1.5493 - val_accuracy: 0.5355\n","Epoch 10/20\n","721/721 [==============================] - 1645s 2s/step - loss: 0.9935 - accuracy: 0.6363 - val_loss: 1.5289 - val_accuracy: 0.4968\n"]}],"source":["# Pretrained MobileNetV2 model\n","base_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))\n","\n","# Unfreeze some layers of the pretrained model\n","for layer in base_model.layers[-20:]:\n","    layer.trainable = True\n","\n","# Build the new model\n","base_output = base_model.output\n","flatten = layers.Flatten()(base_output)\n","dense1 = layers.Dense(128, activation='relu')(flatten)\n","batch_norm = layers.BatchNormalization()(dense1)\n","dropout = layers.Dropout(0.5)(batch_norm)\n","final_output = layers.Dense(len(Classes), activation='softmax')(dropout)\n","\n","new_model = tf.keras.models.Model(inputs=base_model.input, outputs=final_output)\n","\n","# Learning rate scheduling\n","lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n","    initial_learning_rate=1e-3,\n","    decay_steps=10000,\n","    decay_rate=0.9,\n","    staircase=True\n",")\n","\n","# Compile the model\n","new_model.compile(\n","    loss=\"sparse_categorical_crossentropy\",\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n","    metrics=[\"accuracy\"]\n",")\n","\n","# EarlyStopping to avoid overfitting\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n","\n","# Train the model with data augmentation\n","history = new_model.fit(\n","    train_generator,\n","    epochs=20,\n","    validation_data=validation_generator,\n","    callbacks=[early_stopping]\n",")"]},{"cell_type":"markdown","metadata":{"id":"Ab33qb0ksh6j"},"source":["## **Evaluate the model**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":423684,"status":"ok","timestamp":1719484510366,"user":{"displayName":"Satyam Kulkarni","userId":"17502473525507827216"},"user_tz":-330},"id":"vA-OKsE3sgwY","outputId":"4cd702aa-8fdf-48a3-c4e1-a2c6dd27fe83"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 7066 images belonging to 7 classes.\n","221/221 [==============================] - 1541s 7s/step - loss: 1.3053 - accuracy: 0.5624\n"]},{"data":{"text/plain":["[1.3053081035614014, 0.5624115467071533]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Final validation data generator\n","test_generator = validation_datagen.flow_from_directory(\n","    validation_dir,\n","    target_size=(img_size, img_size),\n","    batch_size=batch_size,\n","    class_mode='sparse',\n","    classes=Classes\n",")\n","\n","# Evaluate the model using the final validation data\n","new_model.evaluate(test_generator)"]},{"cell_type":"markdown","metadata":{"id":"G5y5IULh7FDu"},"source":["**Save the model**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":457,"status":"ok","timestamp":1719484510811,"user":{"displayName":"Satyam Kulkarni","userId":"17502473525507827216"},"user_tz":-330},"id":"FN_-xsRsAzwX","outputId":"4a163a5b-71d4-4585-b99a-14ddab16f2d1"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}],"source":["# Save the model\n","new_model.save('emotion_recognition_model.h5')"]},{"cell_type":"markdown","metadata":{"id":"Y64eVOsN246v"},"source":["## **Real-time Prediction Code**"]},{"cell_type":"code","source":["!pip install flask flask-ngrok tensorflow keras opencv-python-headless"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hui9Mx9qZ_5m","executionInfo":{"status":"ok","timestamp":1719567658143,"user_tz":-330,"elapsed":9226,"user":{"displayName":"Satyam Kulkarni","userId":"17502473525507827216"}},"outputId":"ce0fb811-d0b7-4201-cb92-6f32bcdcea4f"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n","Collecting flask-ngrok\n","  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.3)\n","Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\n","Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\n","Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.31.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.5)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2024.6.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n","Installing collected packages: flask-ngrok\n","Successfully installed flask-ngrok-0.0.25\n"]}]},{"cell_type":"code","source":["from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode\n","import requests\n","import io\n","from PIL import Image\n","from PIL import Image\n","import numpy as np\n","from tensorflow.keras.models import load_model\n","\n","def take_photo(filename='photo.jpg', quality=0.8):\n","    js = Javascript('''\n","    async function takePhoto(quality) {\n","        const div = document.createElement('div');\n","        const capture = document.createElement('button');\n","        capture.textContent = 'Capture';\n","        div.appendChild(capture);\n","\n","        const video = document.createElement('video');\n","        video.style.display = 'block';\n","        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","        document.body.appendChild(div);\n","        div.appendChild(video);\n","        video.srcObject = stream;\n","        await video.play();\n","\n","        // Resize the output to be more manageable.\n","        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","        // Wait for Capture to be clicked.\n","        await new Promise((resolve) => capture.onclick = resolve);\n","\n","        const canvas = document.createElement('canvas');\n","        canvas.width = video.videoWidth;\n","        canvas.height = video.videoHeight;\n","        canvas.getContext('2d').drawImage(video, 0, 0);\n","        stream.getTracks().forEach(track => track.stop());\n","        div.remove();\n","\n","        // Convert the canvas to base64 encoded image data\n","        const imageData = canvas.toDataURL('image/jpeg', quality);\n","\n","        // Return the base64 encoded image data\n","        return imageData;\n","    }\n","    ''')\n","    display(js)\n","    data = eval_js('takePhoto({})'.format(quality))\n","\n","    # Convert the base64 encoded image data to bytes and save it to a file\n","    binary = b64decode(data.split(',')[1])\n","    with open(filename, 'wb') as f:\n","        f.write(binary)\n","\n","    return filename\n","\n","filename = take_photo()\n","print('Saved to {}'.format(filename))\n","\n","# Display the captured image\n","img = Image.open(filename)\n","img.thumbnail((256, 256))\n","img.show()\n","\n","\n","\n","# Function to preprocess image for model prediction\n","def preprocess_image(filename, target_size=(128, 128)):\n","    img = Image.open(filename)\n","    img = img.resize(target_size)  # Resize to match model's expected input size\n","    img = np.array(img) / 255.0  # Normalize pixel values to [0, 1]\n","    img = np.expand_dims(img, axis=0)  # Add batch dimension\n","    return img\n","\n","# Function to predict emotion using loaded model\n","def predict_emotion(filename, model):\n","    emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']  # Assuming these are your model's output classes\n","    img = preprocess_image(filename, target_size=model.input_shape[1:3])\n","    prediction = model.predict(img)\n","    emotion_label = emotions[np.argmax(prediction)]\n","    confidence = np.max(prediction)\n","    return emotion_label, confidence\n","\n","if __name__ == \"__main__\":\n","    # Load the pre-trained model\n","    model_path = '/content/drive/MyDrive/Projects/Deep Learning for Computer Vision/emotion_recognition_model.h5'\n","    model = load_model(model_path)\n","\n","    # Example filename of captured photo\n","    filename = 'photo.jpg'\n","\n","    # Predict emotion using the captured photo\n","    emotion_label, confidence = predict_emotion(filename, model)\n","\n","    # Print prediction results\n","    print(f\"Predicted Emotion: {emotion_label}\")\n","    print(f\"Confidence: {confidence}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"aUaqLu_dJY5R","executionInfo":{"status":"ok","timestamp":1719567723293,"user_tz":-330,"elapsed":27416,"user":{"displayName":"Satyam Kulkarni","userId":"17502473525507827216"}},"outputId":"79c7b293-6b14-423a-ebd8-934bb8f2e52b"},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function takePhoto(quality) {\n","        const div = document.createElement('div');\n","        const capture = document.createElement('button');\n","        capture.textContent = 'Capture';\n","        div.appendChild(capture);\n","\n","        const video = document.createElement('video');\n","        video.style.display = 'block';\n","        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","        document.body.appendChild(div);\n","        div.appendChild(video);\n","        video.srcObject = stream;\n","        await video.play();\n","\n","        // Resize the output to be more manageable.\n","        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","        // Wait for Capture to be clicked.\n","        await new Promise((resolve) => capture.onclick = resolve);\n","\n","        const canvas = document.createElement('canvas');\n","        canvas.width = video.videoWidth;\n","        canvas.height = video.videoHeight;\n","        canvas.getContext('2d').drawImage(video, 0, 0);\n","        stream.getTracks().forEach(track => track.stop());\n","        div.remove();\n","\n","        // Convert the canvas to base64 encoded image data\n","        const imageData = canvas.toDataURL('image/jpeg', quality);\n","\n","        // Return the base64 encoded image data\n","        return imageData;\n","    }\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saved to photo.jpg\n","1/1 [==============================] - 1s 948ms/step\n","Predicted Emotion: Neutral\n","Confidence: 0.499328076839447\n"]}]},{"cell_type":"markdown","metadata":{"id":"x3bug9rY3kKB"},"source":["## **FLASK APP**"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3623,"status":"ok","timestamp":1719568830490,"user":{"displayName":"Satyam Kulkarni","userId":"17502473525507827216"},"user_tz":-330},"id":"iEQP1ArJaJj1","outputId":"9291d801-9659-42a1-9d75-266858f81af5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install pyngrok"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LsQcwmmEuRTE","executionInfo":{"status":"ok","timestamp":1719568836900,"user_tz":-330,"elapsed":6447,"user":{"displayName":"Satyam Kulkarni","userId":"17502473525507827216"}},"outputId":"62e4bab8-0b5b-43de-cc87-dc068eb16305"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.1.6)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n"]}]},{"cell_type":"code","source":["!ngrok authtoken \"2gorxOm9YlGFf8MoRgQfVzx6RqY_de56fyN9P51du4UayK37\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"motpQpDUK1QA","executionInfo":{"status":"ok","timestamp":1719568836902,"user_tz":-330,"elapsed":24,"user":{"displayName":"Satyam Kulkarni","userId":"17502473525507827216"}},"outputId":"ec9acee8-a5d9-4cad-c4e1-afcb1dffc4ff"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"]}]},{"cell_type":"code","source":["import os\n","import base64\n","from io import BytesIO\n","import numpy as np\n","from PIL import Image\n","from flask import Flask, request, jsonify, render_template\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.applications.resnet50 import preprocess_input\n","from pyngrok import ngrok\n","\n","app = Flask(__name__)\n","\n","# Load your pre-trained TensorFlow/Keras model for emotion detection\n","model_path = '/content/drive/MyDrive/Projects/Deep Learning for Computer Vision/emotion_recognition_model.h5'\n","emotion_model = load_model(model_path)\n","\n","# Function to preprocess image for model prediction\n","def preprocess_image(filename, target_size=(128, 128)):\n","    img = Image.open(filename)\n","    img = img.resize(target_size)  # Resize to match model's expected input size\n","    img = np.array(img) / 255.0  # Normalize pixel values to [0, 1]\n","    img = np.expand_dims(img, axis=0)  # Add batch dimension\n","    return img\n","\n","\n","# Route for the home page\n","@app.route('/', methods=['GET', 'POST'])\n","def home():\n","    return \"\"\"\n","<!DOCTYPE html>\n","<html>\n","  <head>\n","    <title>Webcam Emotion Detection</title>\n","    <style>\n","      body {\n","        font-family: 'Poppins', sans-serif;\n","        background-color: #f9f9f9;\n","        margin: 0;\n","        padding: 0;\n","      }\n","      .container {\n","        max-width: 800px;\n","        margin: 0 auto;\n","        padding: 40px;\n","        background-color: #ffffff;\n","        border-radius: 10px;\n","        box-shadow: 0 10px 20px rgba(0, 0, 0, 0.1);\n","        text-align: center;\n","      }\n","      h1 {\n","        color: #007BFF;\n","        font-size: 36px;\n","        margin-bottom: 20px;\n","      }\n","      video {\n","        width: 100%;\n","        height: auto;\n","        margin-bottom: 20px;\n","      }\n","      #capture {\n","        background-color: #007BFF;\n","        color: #fff;\n","        padding: 15px 30px;\n","        border: none;\n","        border-radius: 5px;\n","        cursor: pointer;\n","        font-size: 20px;\n","        transition: background-color 0.3s ease;\n","      }\n","      #capture:hover {\n","        background-color: #0056b3;\n","      }\n","      p#emotion {\n","        margin-top: 20px;\n","        font-size: 24px;\n","        color: #007BFF;\n","      }\n","    </style>\n","  </head>\n","  <body>\n","    <div class=\"container\">\n","      <h1>Webcam Emotion Detection</h1>\n","      <video id=\"video\" width=\"640\" height=\"480\" autoplay></video>\n","      <br>\n","      <button id=\"capture\">Capture</button>\n","      <canvas id=\"canvas\" width=\"640\" height=\"480\" style=\"display:none;\"></canvas>\n","      <p id=\"emotion\"></p>\n","    </div>\n","\n","    <script>\n","        const video = document.getElementById('video');\n","        const canvas = document.getElementById('canvas');\n","        const captureButton = document.getElementById('capture');\n","        const emotionDisplay = document.getElementById('emotion');\n","\n","        // Use navigator.mediaDevices.getUserMedia to access webcam\n","        navigator.mediaDevices.getUserMedia({ video: true })\n","            .then(function(stream) {\n","                video.srcObject = stream;\n","            })\n","            .catch(function(err) {\n","                console.error(\"Error accessing webcam: \" + err);\n","            });\n","\n","        // Capture button click event\n","        captureButton.addEventListener('click', function() {\n","            const context = canvas.getContext('2d');\n","            context.drawImage(video, 0, 0, canvas.width, canvas.height);\n","\n","            // Convert canvas to base64 image data\n","            const image = canvas.toDataURL('image/jpeg', 0.8);\n","\n","            // Send image data to Flask endpoint for prediction\n","            fetch('/predict', {\n","                method: 'POST',\n","                headers: {\n","                    'Content-Type': 'application/json'\n","                },\n","                body: JSON.stringify({ image: image })\n","            })\n","            .then(response => response.json())\n","            .then(data => {\n","                emotionDisplay.textContent = 'Detected Emotion: ' + data.emotion;\n","            })\n","            .catch(error => {\n","                console.error('Error:', error);\n","            });\n","        });\n","    </script>\n","  </body>\n","</html>\n","    \"\"\"\n","\n","# Prediction endpoint\n","@app.route('/predict', methods=['POST'])\n","def predict():\n","    image_data = request.json['image']\n","\n","    # Save base64 image to a temporary file\n","    temp_img_filename = 'temp_img.jpg'\n","    with open(temp_img_filename, 'wb') as f:\n","        f.write(base64.b64decode(image_data.split(',')[1]))\n","\n","    # Preprocess the image\n","    img_array = preprocess_image(temp_img_filename, target_size=(128, 128))\n","\n","    # Remove the temporary image file\n","    os.remove(temp_img_filename)\n","\n","    # Predict emotion\n","    prediction = emotion_model.predict(img_array)\n","    emotion_label = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral'][np.argmax(prediction)]\n","\n","    return jsonify({'emotion': emotion_label})\n","\n","\n","if __name__ == '__main__':\n","    # Start ngrok tunnel\n","    public_url = ngrok.connect(addr=\"5000\")\n","    print(\" * Running on\", public_url)\n","    app.run()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6z9l1SxGuHo7","executionInfo":{"status":"ok","timestamp":1719569014813,"user_tz":-330,"elapsed":177926,"user":{"displayName":"Satyam Kulkarni","userId":"17502473525507827216"}},"outputId":"0dba909b-c320-45fb-95cb-62a5d60101ff"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":[" * Running on NgrokTunnel: \"https://f9c5-34-72-137-10.ngrok-free.app\" -> \"http://localhost:5000\"\n"," * Serving Flask app '__main__'\n"," * Debug mode: off\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n"," * Running on http://127.0.0.1:5000\n","INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n","INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:01:00] \"GET / HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:01:02] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 988ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:01:26] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 40ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:01:29] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 39ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:01:43] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 40ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:01:45] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 63ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:01:47] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 60ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:01:49] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 38ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:01:54] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 39ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:01:56] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 42ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:01:57] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 42ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:02:08] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 58ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:02:15] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 36ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:02:16] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 48ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:02:18] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 38ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:02:20] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 37ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:02:29] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 39ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:02:32] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 40ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:02:35] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 38ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:02:41] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 36ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:02:44] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 39ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:02:46] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 70ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:02:52] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 41ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:02:54] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 37ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:02:57] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 54ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:03:05] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 37ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:03:09] \"POST /predict HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 59ms/step\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [28/Jun/2024 10:03:15] \"POST /predict HTTP/1.1\" 200 -\n"]}]},{"cell_type":"markdown","metadata":{"id":"gCX9965dhzqZ"},"source":["# **Conclusion**"]},{"cell_type":"markdown","metadata":{"id":"Fjb1IsQkh3yE"},"source":["From above, we can infer that model is giving sufficiently accurate results for facial emotions prediction. Accuracy of model further may be increased using more computational power and more training to ML Model.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gIfDvo9L0UH2"},"source":["### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"]}],"metadata":{"colab":{"collapsed_sections":["vncDsAP0Gaoa","FJNUwmbgGyua","w6K7xa23Elo4","yQaldy8SH6Dl","mDgbUHAGgjLW","TnqF7WmjOlGG","VfCC591jGiD4","Ab33qb0ksh6j","Y64eVOsN246v","gCX9965dhzqZ","gIfDvo9L0UH2"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}